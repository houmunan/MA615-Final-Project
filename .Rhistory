tweets_00.df <- parseTweets("tweetsUS_1129_23.json",verbose = FALSE)
tweets_01.df <- parseTweets("tweetsUS_1130_00.json",verbose = FALSE)
tweets_02.df <- parseTweets("tweetsUS_1130_01.json",verbose = FALSE)
tweets_03.df <- parseTweets("tweetsUS_1130_02.json",verbose = FALSE)
tweets_04.df <- parseTweets("tweetsUS_1130_03.json",verbose = FALSE)
tweets_05.df <- parseTweets("tweetsUS_1130_04.json",verbose = FALSE)
tweets_06.df <- parseTweets("tweetsUS_1130_05.json",verbose = FALSE)
tweets_07.df <- parseTweets("tweetsUS_1130_06.json",verbose = FALSE)
tweets_08.df <- parseTweets("tweetsUS_1130_07.json",verbose = FALSE)
tweets_09.df <- parseTweets("tweetsUS_1130_08.json",verbose = FALSE)
tweets_10.df <- parseTweets("tweetsUS_1130_09.json",verbose = FALSE)
tweets_11.df <- parseTweets("tweetsUS_1130_10.json",verbose = FALSE)
tweets_12.df <- parseTweets("tweetsUS_1130_11.json",verbose = FALSE)
tweets_13.df <- parseTweets("tweetsUS_1130_12.json",verbose = FALSE)
tweets_14.df <- parseTweets("tweetsUS_1130_13.json",verbose = FALSE)
tweets_15.df <- parseTweets("tweetsUS_1130_14.json",verbose = FALSE)
tweets_16.df <- parseTweets("tweetsUS_1130_15.json",verbose = FALSE)
tweets_17.df <- parseTweets("tweetsUS_1130_16.json",verbose = FALSE)
tweets_18.df <- parseTweets("tweetsUS_1130_17.json",verbose = FALSE)
tweets_19.df <- parseTweets("tweetsUS_1130_18.json",verbose = FALSE)
tweets_20.df <- parseTweets("tweetsUS_1130_19.json",verbose = FALSE)
tweets_21.df <- parseTweets("tweetsUS_1130_20.json",verbose = FALSE)
tweets_22.df <- parseTweets("tweetsUS_1130_21.json",verbose = FALSE)
tweets_23.df <- parseTweets("tweetsUS_1130_22.json",verbose = FALSE)
tweets_24.df <- parseTweets("tweetsUS_1130_23.json",verbose = FALSE)
tweets_25.df <- parseTweets("tweetsUS_1201_00.json",verbose = FALSE)
tweets_26.df <- parseTweets("tweetsUS_1201_01.json",verbose = FALSE)
tweets_27.df <- parseTweets("tweetsUS_1201_02.json",verbose = FALSE)
tweets_28.df <- parseTweets("tweetsUS_1201_03.json",verbose = FALSE)
library(ROAuth)
library(streamR)
library(twitteR)
options(geonamesUsername="crysclover")
library(geonames)
library(tm)
library(SnowballC)
library(ggplot2)
library(ggvis)
library(ggmap)
library(grid)
library(dplyr)
library(wordcloud)
library(ROAuth)
library(streamR)
library(twitteR)
options(geonamesUsername="crysclover")
library(geonames)
library(tm)
library(SnowballC)
library(ggplot2)
library(ggvis)
library(ggmap)
library(grid)
library(dplyr)
library(wordcloud)
tweets_00.df <- parseTweets("tweetsUS_1129_23.json",verbose = FALSE)
tweets_01.df <- parseTweets("tweetsUS_1130_00.json",verbose = FALSE)
tweets_02.df <- parseTweets("tweetsUS_1130_01.json",verbose = FALSE)
tweets_03.df <- parseTweets("tweetsUS_1130_02.json",verbose = FALSE)
tweets_04.df <- parseTweets("tweetsUS_1130_03.json",verbose = FALSE)
tweets_05.df <- parseTweets("tweetsUS_1130_04.json",verbose = FALSE)
tweets_06.df <- parseTweets("tweetsUS_1130_05.json",verbose = FALSE)
tweets_07.df <- parseTweets("tweetsUS_1130_06.json",verbose = FALSE)
tweets_08.df <- parseTweets("tweetsUS_1130_07.json",verbose = FALSE)
tweets_09.df <- parseTweets("tweetsUS_1130_08.json",verbose = FALSE)
tweets_10.df <- parseTweets("tweetsUS_1130_09.json",verbose = FALSE)
tweets_11.df <- parseTweets("tweetsUS_1130_10.json",verbose = FALSE)
tweets_12.df <- parseTweets("tweetsUS_1130_11.json",verbose = FALSE)
tweets_13.df <- parseTweets("tweetsUS_1130_12.json",verbose = FALSE)
tweets_14.df <- parseTweets("tweetsUS_1130_13.json",verbose = FALSE)
tweets_15.df <- parseTweets("tweetsUS_1130_14.json",verbose = FALSE)
tweets_16.df <- parseTweets("tweetsUS_1130_15.json",verbose = FALSE)
tweets_17.df <- parseTweets("tweetsUS_1130_16.json",verbose = FALSE)
tweets_18.df <- parseTweets("tweetsUS_1130_17.json",verbose = FALSE)
tweets_19.df <- parseTweets("tweetsUS_1130_18.json",verbose = FALSE)
tweets_20.df <- parseTweets("tweetsUS_1130_19.json",verbose = FALSE)
tweets_21.df <- parseTweets("tweetsUS_1130_20.json",verbose = FALSE)
tweets_22.df <- parseTweets("tweetsUS_1130_21.json",verbose = FALSE)
tweets_23.df <- parseTweets("tweetsUS_1130_22.json",verbose = FALSE)
tweets_24.df <- parseTweets("tweetsUS_1130_23.json",verbose = FALSE)
tweets_25.df <- parseTweets("tweetsUS_1201_00.json",verbose = FALSE)
tweets_26.df <- parseTweets("tweetsUS_1201_01.json",verbose = FALSE)
tweets_27.df <- parseTweets("tweetsUS_1201_02.json",verbose = FALSE)
tweets_28.df <- parseTweets("tweetsUS_1201_03.json",verbose = FALSE)
tweets_00.df[,"time_mark"] <- as.integer(00)
tweets_01.df[,"time_mark"] <- as.integer(01)
tweets_02.df[,"time_mark"] <- as.integer(02)
tweets_03.df[,"time_mark"] <- as.integer(03)
tweets_04.df[,"time_mark"] <- as.integer(04)
tweets_05.df[,"time_mark"] <- as.integer(05)
tweets_06.df[,"time_mark"] <- as.integer(06)
tweets_07.df[,"time_mark"] <- as.integer(07)
tweets_08.df[,"time_mark"] <- as.integer(08)
tweets_09.df[,"time_mark"] <- as.integer(09)
tweets_10.df[,"time_mark"] <- as.integer(10)
tweets_11.df[,"time_mark"] <- as.integer(11)
tweets_12.df[,"time_mark"] <- as.integer(12)
tweets_13.df[,"time_mark"] <- as.integer(13)
tweets_14.df[,"time_mark"] <- as.integer(14)
tweets_15.df[,"time_mark"] <- as.integer(15)
tweets_16.df[,"time_mark"] <- as.integer(16)
tweets_17.df[,"time_mark"] <- as.integer(17)
tweets_18.df[,"time_mark"] <- as.integer(18)
tweets_19.df[,"time_mark"] <- as.integer(19)
tweets_20.df[,"time_mark"] <- as.integer(20)
tweets_21.df[,"time_mark"] <- as.integer(21)
tweets_22.df[,"time_mark"] <- as.integer(22)
tweets_23.df[,"time_mark"] <- as.integer(23)
tweets_24.df[,"time_mark"] <- as.integer(24)
tweets_25.df[,"time_mark"] <- as.integer(25)
tweets_26.df[,"time_mark"] <- as.integer(26)
tweets_27.df[,"time_mark"] <- as.integer(27)
tweets_28.df[,"time_mark"] <- as.integer(28)
keep <- c("text","lang","listed_count","geo_enabled","statuses_count","followers_count","favourites_count","friends_count","time_zone","country_code","full_name","place_lat","place_lon","time_mark")
tweets_00.df <- tweets_00.df[,keep]
tweets_01.df <- tweets_01.df[,keep]
tweets_02.df <- tweets_02.df[,keep]
tweets_03.df <- tweets_03.df[,keep]
tweets_04.df <- tweets_04.df[,keep]
tweets_05.df <- tweets_05.df[,keep]
tweets_06.df <- tweets_06.df[,keep]
tweets_07.df <- tweets_07.df[,keep]
tweets_08.df <- tweets_08.df[,keep]
tweets_09.df <- tweets_09.df[,keep]
tweets_10.df <- tweets_10.df[,keep]
tweets_11.df <- tweets_11.df[,keep]
tweets_12.df <- tweets_12.df[,keep]
tweets_13.df <- tweets_13.df[,keep]
tweets_14.df <- tweets_14.df[,keep]
tweets_15.df <- tweets_15.df[,keep]
tweets_16.df <- tweets_16.df[,keep]
tweets_17.df <- tweets_17.df[,keep]
tweets_18.df <- tweets_18.df[,keep]
tweets_19.df <- tweets_19.df[,keep]
tweets_20.df <- tweets_20.df[,keep]
tweets_21.df <- tweets_21.df[,keep]
tweets_22.df <- tweets_22.df[,keep]
tweets_23.df <- tweets_23.df[,keep]
tweets_24.df <- tweets_24.df[,keep]
tweets_25.df <- tweets_25.df[,keep]
tweets_26.df <- tweets_26.df[,keep]
tweets_27.df <- tweets_27.df[,keep]
tweets_28.df <- tweets_28.df[,keep]
tweets.df <- rbind(tweets_00.df,tweets_01.df,tweets_02.df,tweets_03.df,tweets_04.df,
tweets_05.df,tweets_06.df,tweets_07.df,tweets_08.df,tweets_09.df,
tweets_10.df,tweets_11.df,tweets_12.df,tweets_13.df,tweets_14.df,
tweets_15.df,tweets_16.df,tweets_17.df,tweets_18.df,tweets_19.df,
tweets_20.df,tweets_21.df,tweets_22.df,tweets_23.df,tweets_24.df,
tweets_25.df,tweets_26.df,tweets_27.df,tweets_28.df)
# back up: write.csv(tweets.df,"tweets.df.csv")
#Remove tweets with no geo_enabled and not English
tweets.df <- tweets.df[tweets.df$lang=="en",]
tweets.df <- tweets.df[tweets.df$geo_enabled==TRUE,]
#clean up extreme values
tweets.df <- tweets.df[tweets.df$place_lat >25 & tweets.df$place_lat <50 & tweets.df$place_lon > -125 & tweets.df$place_lon< -66,]
#Filter different places
ca <- data.frame(filter(tweets.df, grepl(', CA', full_name)))
il <- data.frame(filter(tweets.df, grepl(', IL', full_name)))
ma <- data.frame(filter(tweets.df, grepl(', MA', full_name)))
ny <- data.frame(filter(tweets.df, grepl(', NY', full_name)))
dc <- data.frame(filter(tweets.df, grepl(', DC', full_name)))
#Adjust based on time zone
ma <- ma %>% filter(time_mark >= 1 & time_mark <= 24)
ny <- ny %>% filter(time_mark >= 1 & time_mark <= 24)
dc <- dc %>% filter(time_mark >= 1 & time_mark <= 24)
ca <- ca %>% filter(time_mark >= 4 & time_mark <= 27)
ca$time_mark <- ca$time_mark - 3
il <- il %>% filter(time_mark >= 2 & time_mark <= 25)
il$time_mark <- il$time_mark - 1
qmplot(place_lon, place_lat, data = ma, colour = I('purple'), size = I(3), mapcolor = "color")
qmplot(place_lon, place_lat, data = ma, colour = I('purple'), size = I(3), mapcolor = "color", main="MA")
qmplot(place_lon, place_lat, data = ny, colour = I('red'), size = I(3), mapcolor = "color")
qmplot(place_lon, place_lat, data = il, colour = I('green'), size = I(3), mapcolor = "color")
qmplot(place_lon, place_lat, data = ca, colour = I('blue'), size = I(3), mapcolor = "color")
library(plyr)
library(stringr)
library(dplyr)
library(ggplot2)
library(tm)
library(wordcloud)
library(gridExtra)
library(topicmodels)
#time_mark start from 0
ca[,"time_mark"] <- ca[,"time_mark"]-1
ny[,"time_mark"] <- ny[,"time_mark"]-1
il[,"time_mark"] <- il[,"time_mark"]-1
dc[,"time_mark"] <- dc[,"time_mark"]-1
ma[,"time_mark"] <- ma[,"time_mark"]-1
#add new variable state
ca[,"state"] <- rep("CA",nrow(ca))
ny[,"state"] <- rep("NY",nrow(ny))
il[,"state"] <- rep("IL",nrow(il))
dc[,"state"] <- rep("DC",nrow(dc))
ma[,"state"] <- rep("MA",nrow(ma))
#get city name
ma$full_name <- as.character(ma$full_name)
ny$full_name <- as.character(ny$full_name)
il$full_name <- as.character(il$full_name)
ca$full_name <- as.character(ca$full_name)
dc$full_name <- as.character(dc$full_name)
get_city <- function(x){
city_name <- c()
name <- strsplit(x$full_name,",")
for(i in 1:nrow(x)){
city_name <- c(city_name,name[[i]][1])
}
return(city_name)
}
ma[,"city"] <- factor(get_city(ma))
ny[,"city"] <- factor(get_city(ny))
il[,"city"] <- factor(get_city(il))
ca[,"city"] <- factor(get_city(ca))
dc[,"city"] <- factor(get_city(dc))
#combine together
total <- rbind(rbind(rbind(rbind(ca,ny),il),dc),ma)
total$lang <- as.character(total$lang)
total <- filter(total,lang=="en")
#factor time_mark
ma[,"time_mark"] <- factor(ma[,"time_mark"])
ca[,"time_mark"] <- factor(ca[,"time_mark"])
ny[,"time_mark"] <- factor(ny[,"time_mark"])
dc[,"time_mark"] <- factor(dc[,"time_mark"])
il[,"time_mark"] <- factor(il[,"time_mark"])
total_time_count <- data.frame("Num_tweet"=c(summary(ma$time_mark),summary(ca$time_mark),
summary(ny$time_mark),summary(dc$time_mark),summary(il$time_mark)))
total_time_count[,"time"] <- c(0:23,0:23,0:23,0:23,0:23)
total_time_count[,"state"] <- c(rep("MA",24),rep("CA",24),rep("NY",24),
rep("DC",24),rep("IL",24))
ggplot(total_time_count,aes(x=time,y=Num_tweet))+geom_point(aes(color=state))+
geom_line(aes(color=state))+ylab("tweet count")
# tweet number for every city in state
count <- summary(ma$city)[1:15];ma_city_count <- as.data.frame(count)
count <- summary(ny$city)[1:15];ny_city_count <- as.data.frame(count)
count <- summary(il$city)[1:15];il_city_count <- as.data.frame(count)
count <- summary(ca$city)[1:15];ca_city_count <- as.data.frame(count)
macityplot <- ggplot(ma_city_count,aes(reorder(rownames(ma_city_count),count),count))+
geom_bar(stat = "identity")+coord_flip()+xlab("City")+ggtitle("MA")
nycityplot <- ggplot(ny_city_count,aes(reorder(rownames(ny_city_count),count),count))+
geom_bar(stat = "identity")+coord_flip()+xlab("City")+ggtitle("NY")
grid.arrange(macityplot, nycityplot, ncol=2)
ilcityplot <- ggplot(il_city_count,aes(reorder(rownames(il_city_count),count),count))+
geom_bar(stat = "identity")+coord_flip()+xlab("City")+ggtitle("IL")
cacityplot <- ggplot(ca_city_count,aes(reorder(rownames(ca_city_count),count),count))+
geom_bar(stat = "identity")+coord_flip()+xlab("City")+ggtitle("CA")
grid.arrange(ilcityplot, cacityplot, ncol=2)
#the most tweets in all five states
get_city2 <- function(x){
city_name <- c()
name <- strsplit(x,",")
for(i in 1:length(x)){
city_name <- c(city_name,name[[i]][1])
}
return(city_name)
}
get_state2 <- function(x){
state_name <- c()
name <- strsplit(x,",")
for(i in 1:length(x)){
state_name <- c(state_name,name[[i]][2])
}
return(state_name)
}
total$full_name <- as.factor(total$full_name)
count <- summary(total$full_name)[1:15];total_city_count <- as.data.frame(count)
total_city_count[,"cs"] <- row.names(total_city_count)
total_city_count[,"city"] <- get_city2(as.character(total_city_count[,"cs"]))
total_city_count[,"state"] <- get_state2(as.character(total_city_count[,"cs"]))
ggplot(total_city_count,aes(reorder(city,count),count))+
geom_bar(stat = "identity",aes(fill=state))+coord_flip()+xlab("City")
#usual test for the mean density between IL and MA, first test variance
den_IL <- filter(total_time_count,state=="IL")[,"density"]
den_MA <- filter(total_time_count,state=="MA")[,"density"]
var.test(den_IL,den_MA)
View(total_time_count)
#tweet density
total_time_count[,"population"] <- c(rep(6.745*10^6,24),rep(38.8*10^6,24),
rep(8.406*10^6,24),rep(658893,24),rep(12.88*10^6,24))
total_time_count[,"density"] <- total_time_count$Num_tweet/total_time_count$population
ggplot(total_time_count,aes(x=time,y=density))+geom_point(aes(color=state))+
geom_line(aes(color=state))
#usual test for the mean density between IL and MA, first test variance
den_IL <- filter(total_time_count,state=="IL")[,"density"]
den_MA <- filter(total_time_count,state=="MA")[,"density"]
var.test(den_IL,den_MA)
t.test(den_IL,den_MA)
#permutation test for the mean density between IL and MA
par(mfrow=c(1,2))
hist(den_IL)
hist(den_MA)
data <- c(den_IL, den_MA)
l1 <- length(den_IL)
l2 <- length(den_MA)
lt <- l1+l2
test.diff <- mean(den_IL) - mean(den_MA)
it <- function(n){
M = NULL
for(i in 1:n){
s = sample(data, lt, FALSE)
m1 = mean(s[1:l1]) - mean(s[(l1+1):lt])
M = c(M,m1)
}
return(M)
}
examples <- it(10000)
par(mfrow=c(1,1))
hist(examples, col = "pink", breaks = 100, main="Random Permutations", xlab="")
abline(v = test.diff, col = "green", lwd = 4)
(sum(examples<test.diff)+sum(examples>-test.diff))/(10000) #two tail p-value
#five count number
pairs(~listed_count+statuses_count+followers_count+favourites_count+friends_count, data=total)
cor(total[,c(3,4,5,6,7)])
View(total)
cor(total[,c(3,5,6,7,8)])
#followers and listed have relation
model_fl <- lm(followers_count~listed_count,data=total)
summary(model_fl)
ggplot(total,aes(x=listed_count,y=followers_count))+geom_point()+geom_smooth(method = "lm")
par(mfrow=c(1,2))
plot(model_fl,1)
plot(model_fl,2)
#Sample for wordcloud
tweets_sample.df <- tweets_10.df
tweets_sample.df <- tweets_sample.df[tweets_sample.df$lang=="en",]
tweets_sample.df <- tweets_sample.df[tweets_sample.df$geo_enabled==TRUE,]
tweets_sample.df <- tweets_sample.df[tweets_sample.df$place_lat >25 &
tweets_sample.df$place_lat <50 & tweets_sample.df$place_lon > -125 &
tweets_sample.df$place_lon< -66,]
tweets_sample.df$geo_enabled <- NULL
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(tweets_sample.df$text))
# remove anything other than English letters or space(!!!)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove stopwords
myStopwords <- c(stopwords('english'),"use", "see", "used", "via", "amp")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# Build Term Document Matrix
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
term.freq <- rowSums(as.matrix(tdm))
term.freq2 <- subset(term.freq, term.freq >= 200)
df <- data.frame(term = names(term.freq2), freq = term.freq2)
par(mfrow=c(1,1))
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") +
ylab("Count") + coord_flip() + theme(axis.text=element_text(size=7))
m <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
# colors
pal <- brewer.pal(8, "Dark2")
# plot word cloud
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 800,
random.order = F, colors = pal, max.words = 60)
# remove sparse terms
tdm2 <- removeSparseTerms(tdm,sparse = 0.965)
# showing the words that are left gor the analysis
print(dimnames(tdm2)$Terms)
m2 <- as.matrix(tdm2)
m3 <- t(m2)   # transpose the matrix to cluster documents
set.seed(122)   # set a fixed random seed
k <- 6   # number of clusters
kmeansResult <- kmeans(m3,k)
round(kmeansResult$centers, digits = 3)   # cluster centers
for(i in 1:k){
cat(paste("cluster ", i, ": ", sep = ""))
s <- sort(kmeansResult$centers[i,],decreasing = T)
cat(names(s)[1:5],"\n")
#print the tweet of every cluster
}
# cluster terms
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method = "complete")
# show cluster dendrogram
p <- plot(fit, xlab="")
p <- rect.hclust(fit, k=6)
tweets_00.df <- parseTweets("data/tweetsUS_1129_23.json",verbose = FALSE)
tweets_00.df <- parseTweets("/data/tweetsUS_1129_23.json",verbose = FALSE)
tweets_00.df <- parseTweets("data\tweetsUS_1129_23.json",verbose = FALSE)
tweets_00.df <- parseTweets("~\data\tweetsUS_1129_23.json",verbose = FALSE)
tweets_00.df <- parseTweets("~data\tweetsUS_1129_23.json",verbose = FALSE)
tweets_00.df <- parseTweets("~data/tweetsUS_1129_23.json",verbose = FALSE)
tweets_00.df <- parseTweets("~/data/tweetsUS_1129_23.json",verbose = FALSE)
tweets_00.df <- parseTweets(".../data/tweetsUS_1129_23.json",verbose = FALSE)
tweets_00.df <- parseTweets("../data/tweetsUS_1129_23.json",verbose = FALSE)
tweets_00.df <- parseTweets("data/tweetsUS_1129_23.json",verbose = FALSE)
tweets_00.df <- parseTweets("tweetsUS_1129_23.json",verbose = FALSE)
par(mfrow=c(1,2))
qmplot(place_lon, place_lat, data = ma, colour = I('purple'), size = I(3), mapcolor = "color")
qmplot(place_lon, place_lat, data = ny, colour = I('red'), size = I(3), mapcolor = "color")
qmplot(place_lon, place_lat, data = ma, colour = I('purple'), size = I(3), mapcolor = "color")
qmplot(place_lon, place_lat, data = ny, colour = I('red'), size = I(3), mapcolor = "color")
qmplot(place_lon, place_lat, data = il, colour = I('green'), size = I(3), mapcolor = "color", main="IL")
qmplot(place_lon, place_lat, data = ma, colour = I('purple'), size = I(3),
mapcolor = "color", main="MA")
qmplot(place_lon, place_lat, data = ny, colour = I('red'), size = I(3),
mapcolor = "color", main="NY")
tweets_25.df <- parseTweets("./data/tweetsUS_1201_00.json",verbose = FALSE)
qmplot(place_lon, place_lat, data = ma, colour = I('purple'), size = I(3),
mapcolor = "color", main="MA")
qmplot(place_lon, place_lat, data = ny, colour = I('red'), size = I(3),
mapcolor = "color", main="NY")
qmplot(place_lon, place_lat, data = il, colour = I('green'), size = I(3),
mapcolor = "color", main="IL")
qmplot(place_lon, place_lat, data = ca, colour = I('blue'), size = I(3),
mapcolor = "color",main="CA")
shiny::runApp('app')
library(ROAuth)
library(streamR)
library(twitteR)
library(tm)
library(SnowballC)
library(ggplot2)
library(ggvis)
library(ggmap)
library(grid)
library(dplyr)
library(wordcloud)
library(plyr)
library(stringr)
library(dplyr)
library(wordcloud)
library(gridExtra)
library(topicmodels)
tweets_00.df <- parseTweets("./data/tweetsUS_1129_23.json",verbose = FALSE)
tweets_01.df <- parseTweets("./data/tweetsUS_1130_00.json",verbose = FALSE)
